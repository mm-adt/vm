:imagesdir: ./images/processor

== Processor Structures

The programs of the mm-ADT virtual machine are *types*.
From a set of canonical types (_ctypes_), derived types (_dtypes_) of arbitrary complexity can be constructed using instructions from the VM's https://en.wikipedia.org/wiki/Instruction_set_architecture[instruction set architecture].
Every mm-ADT type has a corresponding https://en.wikipedia.org/wiki/Diagram_(category_theory)[diagrammatic] representation that is https://en.wikipedia.org/wiki/Isomorphism[isomorphic] to a directed labeled *type graph* composed of _type_-vertices and _instruction_-edges.

A program's type graph is the https://en.wikipedia.org/wiki/Intermediate_representation[intermediate representation] used by the mm-ADT VM to not only link types (encode), but also to compile them (transform/optimize).
At execution time, values propagate through the type graph and generate a parallel, homomorphic image of the types as values in the *value graph*, where the resultant structure of an mm-ADT computation is the *obj graph*, where

\[
\texttt{obj} = (\texttt{type} \times \texttt{q}) + (\texttt{value} \times \texttt{q}).
\]

image::process-stack.png[float="right",width=200]

Type composition, compilation, and evaluation are carried out by mm-ADT compliant *processors*.
Processors ground the mm-ADT VM to the underlying physical computer (whether on a single machine, via multiple threads, or across compute cluster), where, at the bottom of this _process stack_, the natural world's physics provides the baseline dynamics (the fundamental ground of the computation).

This section details the specifics of the relationships between types, values, and processors.

=== Processes

Processors are used in the following three situations:

. *Composition*: (https://en.wikipedia.org/wiki/Type_inference[type inference]).
. *Compilation*: (https://en.wikipedia.org/wiki/Program_optimization[type optimization]). (https://en.wikipedia.org/wiki/Fixed_point_%28mathematics%29[fix point]).
. *Evaluation*: (https://en.wikipedia.org/wiki/Execution_(computing)[type enumeration]).

=== Algebraic Actions

A thread of execution in the VM maintains two primary references: 1.) a reference to an `obj` and 2.) a reference to an `inst`.
Via the juxtaposition of `obj` and `inst` another `obj` is realized.
When a VM process refers to the `obj` \$a\$ and the instruction `inst` \$i\$, then the VM will perform one of the following operations:

. If the vertex \$a\$ has no outgoing \$i\$-labeled edge, then the VM _applies_ \$a\$ to \$i\$ to yield the edge \$a \to_i b\$ and arrive at \$b\$ (*compute*).
. If the vertex \$a\$ has an outgoing \$i\$-labeled edge, then the VM _traverses_ the edge \$a \to_i b\$ to arrive at \$b\$ (*memoize*).

The first situation is computing via function https://en.wikipedia.org/wiki/Evaluation_strategy[evaluation] (save space).
The second situation leverages https://en.wikipedia.org/wiki/Memoization[memoization] to avoid recomputing (save time).
These two situations offer three high-level perspectives on the `obj` graph.

. *Mathematically*: The `obj` graph has an infinite number of `obj` vertices connected to each other by edges labeled from the infinite https://en.wikipedia.org/wiki/Instruction_set_architecture[instruction set architecture] `inst`.
From this perspective, computing is traversal (i.e. https://en.wikipedia.org/wiki/Lookup_table[look-up]) as the `obj` graph is fully materialized.
. *Theoretically*: The `obj` graph is manifested as computations proceed where, if \$a \in \tt{obj}\$ and \$i \in \tt{i\nst}\$, then any binary operation \$ai\$ that has already been evaluated already exists in the `obj` graph and as such, can be traversed.
. *Physically*: The `obj` graph is a dynamic entity that expands and contracts given the resource constraints of the underlying physical machines supporting its manifestation across all levels of the https://en.wikipedia.org/wiki/Memory_hierarchy[memory hierarchy], where https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)[garbage collection] prunes the graph and computation grows the graph.

==== Type Specification

An mm-ADT *program* in an mm-ADT *type*.
In the type graph (a subgraph of the `obj` graph) a type is denoted by a vertex (an _ungrounded_ vertex).
That vertex is the type's *range*.
The type's *definition* is encoded in the directed, https://en.wikipedia.org/wiki/Deterministic_automaton[deterministic] path that ends at a vertex with no outgoing edges.
The resultant vertex is a _root_ vertex and is the type's *domain*.
If the type definition's path length is 0, then the domain and the range are equal, and the type is a ctype (a canonical type).
If the path length is greater than 0, then the directed binary edges of the path are labeled with instructions from `inst`.
This construction is abstractly represented in the diagram below.


image::../processor/type-path.png[align=center,width=800]

The type graph forms the central structure upon by which various VM processes are enacted.
These processes include type/program specification, compilation, optimization, and ultimately, via a https://en.wikipedia.org/wiki/Homomorphism[homomorphism] from the type graph to the value graph, evaluation.
Given finite computing resources, the type graph does not exist eternally in a static form ready-made.
No, instead, subgraphs of it must be generated.
This is accomplished via an action of `inst` monoid on the set `inst^*^` (the https://en.wikipedia.org/wiki/Kleene_star[Kleene star] closure of `inst`).
For instance, in `mmlang` the user juxtaposes a ctype (domain) and an `inst` to construct a dtype.
That dtype is juxtaposed with another `inst` to yield another dtype so forth until a desired type is reached.

\[
\texttt{range} = ((((((\texttt{domain} \cdot \texttt{inst}_0) \cdot \texttt{inst}_1) \cdot \texttt{inst}_2) \ldots) \cdot \texttt{inst}_{n-2}) \cdot \texttt{inst}_{n-1}).
\]

In general, the action of an `inst` on a type is the function \[
\texttt{inst}: T \to T, \]
where if \$a \in \tt{i\nst}\$, then \[
a(x) = xa.
\]

Said plainly, instructions in `inst` act on types by concatenating themselves to the type definition.
Thus, algebraically, a type is an element of the https://en.wikipedia.org/wiki/Free_algebra[free] inst monoid rooted at a ctype.

==== Type Compilation

===== obj-Modules

.Modules
****
A https://en.wikipedia.org/wiki/Module_(mathematics)[module] for a group \$(A,+\_A,\mathbf{0}_A)\$ is a ring \$(X, +_X ,\ast_X, \mathbf{0}_X, \mathbf{1}_X )\$ such that elements of \$X\$ act on elements of \$A\$ via a function \$\cdot: X \times A \to A\$ called *scalar multiplication*. If \$A = X\$, the action is defined by the group's additive operator. However, modules are useful when \$A \ne X\$ and moreover, when \$A\$ and/or \$X\$ is free. A free \$A\$ leads to the notion of \$A\$-vectors and matrices being operated by \$X\$ scalars, vectors, and matrices. In other words, modules provide a ring theoretic interpretation of various https://en.wikipedia.org/wiki/Linear_algebra[linear algebraic] structures. The axioms for both left and right modules are provided below, where if \$A\$ is an https://en.wikipedia.org/wiki/Abelian_group[abelian group], then \$X\$ is a https://en.wikipedia.org/wiki/Bimodule[bimodule] and both sets of axioms hold.

[.center]
[cols="^1,^1",width=70]
|===
| Left \$X\$-Module Axioms                             | Right \$X\$-Module Axioms

| \$x \cdot (a +_A b) = (x \cdot a) +_A (x \cdot b) \$ | \$(a +_A b) \cdot x = (a \cdot x) +_A (b \cdot x) \$
| \$(x +_X y) \cdot a = (x \cdot a) +_A (y \cdot a) \$ | \$a \cdot (x +_X y) = (a \cdot x) +_A (a \cdot y) \$
| \$(x \ast_X y) \cdot a = x \cdot (y \cdot a)\$       | \$a \cdot (x \ast_X y) = (a \cdot x) \cdot y\$
| \$\mathbf{1}_X \cdot a = a\$                         | \$a \cdot \mathbf{1}_X = a\$
|===
****

====== Polynomial Ring Modules

[.text-center]
`int[int+2[is>0]\*5<44, int+2[is>0]*-6<44, int+2[is>0]*10+7<44]`

[exec]
----
int[int+2[is>0]*5<44, int+2[is>0]*-6<44, int+2[is>0]*10+7<44]
----

image::module-example-2.png[align=center]

The above expression denotes a polynomial ring whose linearly combined terms are elements of the multiplicative monoid. With abuse of notation, the expression below binds the monoidal terms with `+` to emphasize the prototypical polynomial form \$q_0 x^0 + q_1x^1 + q_2x^2\$.

\[
\texttt{int+2[is>0]\*5<44} \;\;+\;\; \texttt{int+2[is>0]*-6<44} \;\;+\;\; \texttt{int+2[is>0]*10+7<44}
\]

Rings support both left and right https://en.wikipedia.org/wiki/Distributive_property[distributivity] such that the following derivation yields the respective equivalence.

\[
\begin{split}
abcg + abdg + abefg &= a \ast (bcg + bdg + befg) \\
&= a \ast b \ast (cg + dg + efg) \\
&= a \ast b \ast (c + d + ef) \ast g \\
\end{split}
\]

Thus `int+2[is>0]` is https://en.wikipedia.org/wiki/Factorization[factored] out on the left and `<44` is factored out on the right.

[.text-center]
`int+2[is>0][\*5,*-6,*10+7]<44`

Again with an abuse of notation to emphasize the lexical structure.

\[
\texttt{int+2[is>0]} \;\ast\; (\texttt{\*5} \;\;+\;\; \texttt{*-6} \;\;+\;\; \texttt{*10+7}) \;\ast\; \texttt{<44}
\]

[exec]
----
int+2[is>0][*5,*-6,*10+7]<44
----

image::module-example-1.png[align=center]

To be certain, both the factored and unfactored forms of the expression return the same result for the same input.

[exec]
----
5 => [int+2[is>0]*5<44, int+2[is>0]*-6<44, int+2[is>0]*10+7<44]
5 => int+2[is>0][*5,*-6,*10+7]<44
----

A progressive _split/merge_ example is provided to better illustrate the intermediate results of the computation.

[exec]
----
5 => -<(int+2[is>0]*5<44, int+2[is>0]*-6<44, int+2[is>0]*10+7<44)
5 => -<(int+2[is>0]*5<44, int+2[is>0]*-6<44, int+2[is>0]*10+7<44)>-

5 => int
5 => int+2
5 => int+2[is>0]
5 => int+2[is>0]-<(*5,*-6,*10+7)
5 => int+2[is>0]-<(*5,*-6,*10+7)>-
5 => int+2[is>0]-<(*5,*-6,*10+7)>-<44
----

====== Type Ringoid Modules

image:type-ringoid-illustration.png[float=left,width=350]

A non-free element is a _zero_-dimensional point. A free element is a _one_-dimensional line. The carrier set of the *type ringoid* is formed from the union the elements of `obj` stream ring's https://en.wikipedia.org/wiki/Free_abelian_group[free additive abelian group] and https://en.wikipedia.org/wiki/Free_monoid[free multiplicative monoid]. This is the _freest_ possible stream ring representation -- a https://en.wikipedia.org/wiki/Free_algebra[free ring]. With two free magmas, the type ringoid's elements are _two_-dimensional planes. One dimension represents multiplication and the other addition. The type ringoid is encoded in `mmlang` as a `,-lst` (additive) with zero or more `;-lst` (multiplicative) terms. The following is an example of a two-dimensional type ringoid element.

[.text-center]
`[[int;+2;[is>0];*5;_;<44],[int;+2;[is>0];*-6;_;<44],[int;+2;[is>0];*10;+7;<44]]`

[exec]
----
[[int;+2;[is>0];*5;_;<44],[int;+2;[is>0];*-6;_;<44],[int;+2;[is>0];*10;+7;<44]]
----

In a manner analogous to polynomials in https://en.wikipedia.org/wiki/Linear_algebra[linear algebra], the free monoids of the polynomial can be organized into a https://en.wikipedia.org/wiki/Matrix_(mathematics)[matrix], where the following equations maintain `,` and `;` tokens to help orient the reader and the multiplicative identity `_` pads rows to ensure a proper \$n \times m\$-matrix.

\begin{bmatrix}
 \tt{int}; & +2; & \tt{[is>0]}; & *5; & \_ ; & <44, \\
 \tt{int}; & +2; & \tt{[is>0]}; & *{-6}; & \_ ; & <44, \\
 \tt{int}; & +2; & \tt{[is>0]}; & *10; & +7 ; & <44 \\
\end{bmatrix}

A left `obj`-module (a row vector) can be factored out of the matrix leaving an expression of the form \$\mathbf{v}^{\top} \mathbf{M}\$.

[.text-center]
`[[int;+2;[is>0]];[[\*5;<44],[*-6;<44],[*10;+7;<44]]]`

[exec]
----
[[int;+2;[is>0]];[[*5;<44],[*-6;<44],[*10;+7;<44]]]
----

\[
\begin{bmatrix}
  \tt{int}; & +2; & \tt{[is>0]}
\end{bmatrix} ;
\begin{bmatrix}
  *5; & \_ ; & <44, \\
  *{-6}; & \_ ; & <44, \\
  *10; & +7 ; & <44 \\
\end{bmatrix}
\]

Similarly, a right `obj`-module https://en.wikipedia.org/wiki/Scalar_(mathematics)[scalar] can be factored out leaving an expression of the form \$\mathbf{v}^{\top} \mathbf{M} u \$.

[.text-center]
`[[int;+2;[is>0]];[\*5,*-6,[*10;+7]];<44]`

[exec]
----
[[int;+2;[is>0]];[*5,*-6,[*10;+7]];<44]
----

\[
\begin{bmatrix}
  \tt{int}; & +2; & \tt{[is>0]}
\end{bmatrix} ;
\begin{bmatrix}
  *5; & \_ ,    \\
  *{-6}; & \_ , \\
  *10; & +7     \\
\end{bmatrix} ;  <44
\]

This fully factored form can be evaluated with `obj`-scalar left multiplication.

\[
\begin{split}
& 5; \begin{bmatrix}\tt{int}; & +2; & \tt{[is>0]} \end{bmatrix} ; & \begin{bmatrix} *5; & \_ , \\ *{-6}; & \_ , \\ *10; & +7 \\ \end{bmatrix} ;  <44  \\
&= \begin{bmatrix}5; & +2; & \tt{[is>0]} \end{bmatrix} ; & \begin{bmatrix} *5; & \_ , \\ *{-6}; & \_ , \\ *10; & +7 \\ \end{bmatrix} ;  <44  \\
&=7 ;  \begin{bmatrix}
*5; & \_ , \\ *{-6}; & \_ , \\ *10; & +7 \\ \end{bmatrix} ;  <44
= & \begin{bmatrix}
35; & \_ , \\ -42; & \_ , \\ 70; & +7 \\ \end{bmatrix} ;  <44
= \begin{bmatrix}
35, \\ -42, \\ 77 \\ \end{bmatrix} ;  <44
=\begin{bmatrix}
\tt{true}, \\ \tt{true}, \\ \tt{false}   \\ \end{bmatrix}
= \begin{bmatrix}
\tt{true}\{ 2 \}, \\ \tt{false}           \\ \end{bmatrix}
\end{split}
\]

[exec]
----
[5;[[int;+2;[is>0]];[*5,*-6,[*10;+7]];<44]]

5-<(int;+2;[is>0];-<(*5,*-6,-<(*10;+7)))
5-<(int;+2;[is>0];-<(*5,*-6,-<(*10;+7)>-)>-;<44)>-
----

Again, to be certain, all three derivations yield the same result for the same input.

[exec]
----
[5;[[int;+2;[is>0];*5;_;<44],[int;+2;[is>0];*-6;_;<44],[int;+2;[is>0];*10;+7;<44]]]
[5;[[int;+2;[is>0]];[[*5;<44],[*-6;<44],[*10;+7;<44]]]]
[5;[[int;+2;[is>0]];[*5,*-6,[*10;+7]];<44]]
----

The linear algebraic type ringoid compartmentalizes the type induced at the individual instruction-level. This is the _absolutely_ freest representation of a ring(oid). This "cellular form" is well suited to manipulation by the processor. At compile-time, factoring a matrix representation can be leveraged for optimization and rewriting. At evaluation runtime, the free type ringoid provides a deconstructed, 2-dimensional https://en.wikipedia.org/wiki/Pipeline[pipeline] architecture that can be partitioned across machines of a cluster and/or threads of a machine.

==== Type Optimization

==== Type Evaluation

A type https://en.wikipedia.org/wiki/Compiler[compiles] a type.
A type https://en.wikipedia.org/wiki/Executable[evaluates] a value.
The `inst` monoid's *type specification* action yields an element in the free `inst` monoid, which, in the `obj` graph, is realized as a path from a range vertex to a domain vertex.
In the example `obj` graph encoding below, the range vertex is the *type path*'s _source_ and the domain vertex is the path's _target_.

image::../processor/type-path.png[align=center,width=800]

During *type evaluation*, the type path is reversed to form the *co-type path*, where the domain vertex is the source and the range vertex is the target.

image::../processor/co-type-path.png[align=center,width=800]

If \$x \in \tt{value}\$, then \$x\$ is propagated along the co-type path, where the \$\tt{domai\n}\$ and \$\tt{rang\e}\$ types perform https://en.wikipedia.org/wiki/Type_system#Dynamic_type_checking_and_runtime_type_information[runtime type checking] and the instructions transform the source \$x\$ value at each step into the resultant \$y\$ value.

\[
y = ((((((((x \cdot \texttt{domain}) \cdot \texttt{inst}_0) \cdot \texttt{inst}_1) \cdot \texttt{inst}_2) \ldots) \cdot \texttt{inst}_{n-2}) \cdot \texttt{inst}_{n-1}) \cdot \texttt{range}).
\]

[exec]
----
int[plus,1][plus,2][plus,3]                                   //<1>
int[plus,1][plus,2][plus,3][path]                             //<2>
1=>int=>[plus,1]=>[plus,2]=>[plus,3]=>int                     //<3>
1=>int[plus,1][plus,2][plus,3][path]                          //<4>
1=>int[plus,1][plus,2][plus,3][path]>-                        //<5>
----
<1> An `int\<=int` type with a type path length of 5.
<2> The co-type path of the previous type encoded in a `;-poly`.
<3> The step-wise `\=>` evaluation of the co-type path.
<4> The step-wise `\=>` evaluation of the co-type path chambered in a `;-poly`.
<5> The evaluation of the `;-poly` simply returns the last path value.

In the `mmlang` example above, the step-wise `\=>` evaluation of the co-type path is in one-to-one correspondence with the mm-ADT VM's execution plan.
The mm-ADT algebras are particular constraints on the most general algebraic specification of mm-ADT: the `obj` magma.

[.text-center]
`1\=>int\=>[plus,1]\=>[plus,2]\=>[plus,3]\=>int`

===== Type Checking

===== Instruction Evaluation

Every mm-ADT instruction denotes a https://en.wikipedia.org/wiki/Unary_function[unary function], but mm-ADT instructions themselves may contain zero, one, or multiple sub-expressions as arguments.
At the mm-ADT type-level, mm-ADT instructions are \$n\$-ary computable relations, where through currying and stream semantics, ultimately, unary functions are realized.

===== n-Ary Instructions

Instructions that have no arguments and which map one input to one output are *nullary instructions*.
For example, `[neg]` (negative/negate) is a nullary instruction in the type `int[neg]` denoting the unary function \[
\begin{array}.
\texttt{neg} &:& \mathbb{N} \rightarrow \mathbb{N} \\ \texttt{neg}(x) &\mapsto& -x.
\end{array}
\]

The *unary instruction* `[plus,2]` in `int[plus,2]` is evaluated by the processor as the unary function \[
\begin{array}.
\texttt{plus_2} &:& \mathbb{N} \rightarrow \mathbb{N} \\ \texttt{plus_2}(x) &\mapsto& x + 2. \end{array}
\]

Instructions can have arguments that are dependent on the incoming `obj` (i.e. the unary function argument).
For instance, the unary instruction `[plus,[mult,3]]` in `int[plus,int[mult,3]]` denotes the unary function \[
\begin{array}.
\texttt{plus_mult_3} &:& \mathbb{N} \rightarrow \mathbb{N} \\ \texttt{plus_mult_3}(x) &\mapsto& x + (x * 3).
\end{array}
\]

Finally, as example instruction when the domain and range differ, `[gt,[plus,[id]]]` in \[
\tt{bool<=int[gt,int[plus,int[id]]]} \]
denotes the unary function \[
\begin{array}.
\texttt{gt_plus_id} &:& \mathbb{N} \rightarrow \{\texttt{true} \cup \texttt{false}\} \\ \texttt{gt_plus_id}(x) &\mapsto& x > (x + x).
\end{array}
\]

===== n-Ary Relations

==== Instruction Classes

===== Map

===== Filter

===== Trace

===== Branch

=== Processor Implementations